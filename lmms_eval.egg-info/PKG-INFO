Metadata-Version: 2.4
Name: lmms_eval
Version: 0.3.0
Summary: A framework for evaluating large multi-modality language models
Author-email: LMMMs-Lab Evaluation Team <lmms-lab@outlook.com>
License: MIT
Project-URL: Homepage, https://lmms-lab.github.io
Project-URL: Repository, https://github.com/EvolvingLMMs-Lab/lmms-eval
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: accelerate>=0.29.1
Requires-Dist: black==24.1.0
Requires-Dist: isort==5.13.2
Requires-Dist: datasets==2.16.1
Requires-Dist: evaluate>=0.4.0
Requires-Dist: httpx==0.23.3
Requires-Dist: jsonlines
Requires-Dist: numexpr
Requires-Dist: numpy==1.26.4
Requires-Dist: peft>=0.2.0
Requires-Dist: pybind11>=2.6.2
Requires-Dist: pytablewriter
Requires-Dist: sacrebleu>=1.5.0
Requires-Dist: scikit-learn>=0.24.1
Requires-Dist: sqlitedict==2.1.0
Requires-Dist: torch>=2.1.0
Requires-Dist: torchvision>=0.16.0
Requires-Dist: timm
Requires-Dist: einops
Requires-Dist: ftfy
Requires-Dist: openai
Requires-Dist: opencv-python-headless
Requires-Dist: av
Requires-Dist: hf_transfer
Requires-Dist: nltk
Requires-Dist: sentencepiece==0.1.99
Requires-Dist: yt-dlp
Requires-Dist: pycocoevalcap
Requires-Dist: tqdm-multiprocess
Requires-Dist: transformers==4.40.0
Requires-Dist: transformers-stream-generator
Requires-Dist: zstandard
Requires-Dist: pillow
Requires-Dist: pyyaml
Requires-Dist: sympy
Requires-Dist: mpmath
Requires-Dist: Jinja2
Requires-Dist: openpyxl
Requires-Dist: loguru
Requires-Dist: hf_transfer
Requires-Dist: tenacity==8.3.0
Requires-Dist: wandb>=0.16.0
Requires-Dist: tiktoken
Requires-Dist: pre-commit
Requires-Dist: pydantic
Requires-Dist: packaging
Requires-Dist: decord
Requires-Dist: zss
Requires-Dist: protobuf==3.20
Requires-Dist: sentence_transformers
Provides-Extra: audio
Requires-Dist: more-itertools; extra == "audio"
Requires-Dist: editdistance; extra == "audio"
Requires-Dist: zhconv; extra == "audio"
Requires-Dist: librosa; extra == "audio"
Requires-Dist: soundfile; extra == "audio"
Provides-Extra: metrics
Requires-Dist: pywsd; extra == "metrics"
Requires-Dist: spacy; extra == "metrics"
Requires-Dist: anls; extra == "metrics"
Requires-Dist: rouge; extra == "metrics"
Requires-Dist: capture_metric; extra == "metrics"
Requires-Dist: Levenshtein; extra == "metrics"
Provides-Extra: gemini
Requires-Dist: google-generativeai; extra == "gemini"
Provides-Extra: reka
Requires-Dist: httpx==0.23.3; extra == "reka"
Requires-Dist: reka-api; extra == "reka"
Provides-Extra: qwen
Requires-Dist: decord; extra == "qwen"
Requires-Dist: qwen_vl_utils; extra == "qwen"
Provides-Extra: mmsearch
Requires-Dist: playwright; extra == "mmsearch"
Requires-Dist: requests; extra == "mmsearch"
Requires-Dist: matplotlib; extra == "mmsearch"
Requires-Dist: duckduckgo_search; extra == "mmsearch"
Requires-Dist: langchain; extra == "mmsearch"
Requires-Dist: langchain-community; extra == "mmsearch"
Requires-Dist: beautifulsoup4; extra == "mmsearch"
Requires-Dist: FlagEmbedding; extra == "mmsearch"
Requires-Dist: rouge; extra == "mmsearch"
Provides-Extra: all
Requires-Dist: gemini; extra == "all"
Requires-Dist: reka; extra == "all"
Requires-Dist: metrics; extra == "all"
Requires-Dist: qwen; extra == "all"
Requires-Dist: mmsearch; extra == "all"
Dynamic: license-file

## Environment Setup

Please refer to https://github.com/EvolvingLMMs-Lab/lmms-eval

```bash
conda create --name exp1 python=3.10
conda install pytorch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 pytorch-cuda=12.1 -c pytorch -c nvidia
pip install flash-attn==2.5.0 --no-build-isolation
pip install -e .
pip install open-clip-torch==2.29.0
```

## Evaluation

We provide example scripts for reproducing results of our method. Raw logs of experimental results are put in 'logs/' directory.

For most datasets, you can get the final scores by replacing `$TASK` with dataset name and running the following command:

```bash
TASK=videomme
python -m accelerate.commands.launch \
    --num_processes=6 \
    -m lmms_eval \
    --model llava_video \
    --model_args pretrained=../model/llava-video,conv_template=qwen_1_5,model_name=llava_qwen,max_frames_num=64\
    --tasks $TASK \
    --batch_size 1 \
    --log_samples \
    --log_samples_suffix llava_video_$TASK \
    --output_path ./logs/
```

```bash
TASK=longvideobench_val_v
python -m accelerate.commands.launch \
    --num_processes=6 \
    -m lmms_eval \
    --model llava_video \
    --model_args pretrained=../model/llava-video,conv_template=qwen_1_5,model_name=llava_qwen,max_frames_num=64\
    --tasks $TASK \
    --batch_size 1 \
    --log_samples \
    --log_samples_suffix llava_video_$TASK \
    --output_path ./logs/
```

For EgoSchema, you need to additionally submit the inference results to the validation server for final scores:


```bash
TASK=egoschema
python -m accelerate.commands.launch \
    --num_processes=6 \
    -m lmms_eval \
    --model llava_video \
    --model_args pretrained=../model/llava-video,conv_template=qwen_1_5,model_name=llava_qwen,max_frames_num=64\
    --tasks $TASK \
    --batch_size 1 \
    --log_samples \
    --log_samples_suffix llava_video_$TASK \
    --output_path ./logs/

# submit to the validation server
python submit_egoschema.py --f logs/submissions/inference_results_egoschema_MC_xxx.json
```
